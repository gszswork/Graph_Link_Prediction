{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOcEJLpBsI16"
   },
   "source": [
    "### Feature Engineering\n",
    "##### It's a simple step, extract all edges in the train.txt document. Then save all edges line by line in edges.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 336 397 478 496 1123 1123 1319 1703 2328 3159 3322 3409 3441 3695\n",
      "\n",
      "1123 1123\n",
      "3269 3269\n",
      "\n",
      "3269 3269\n",
      "3767\n",
      "151 1123\n",
      "336 1123\n",
      "397 1123\n",
      "478 1123\n",
      "496 1123\n",
      "1123 1319\n",
      "1123 1703\n",
      "1123 2328\n",
      "1123 3159\n",
      "1123 3322\n",
      "1123 3409\n",
      "1123 3441\n",
      "1123 3695\n",
      "1259 3269\n",
      "2557 3269\n",
      "2607 3269\n",
      "16034\n",
      "Added 16083  edges.\n"
     ]
    }
   ],
   "source": [
    "# Candidate0: \n",
    "weight_dict = {}\n",
    "nodes = []\n",
    "with open('data/train.txt', 'r') as f, open('data/edges.txt', 'w') as o, open('data/test-public.csv', 'r') as t:\n",
    "    training_data = f.readlines()\n",
    "    for line in training_data:\n",
    "        l = line.strip().split()\n",
    "        for i in range(len(l)):\n",
    "            nodes.append(int(l[i]))\n",
    "            for j in range(i+1, len(l)):\n",
    "                if l[i] == l[j]:\n",
    "                    print(line)\n",
    "                    print(l[i], l[j])\n",
    "                    continue\n",
    "                if (int(l[i]), int(l[j])) not in weight_dict:\n",
    "                    weight_dict[(int(l[i]), int(l[j]))] = 1\n",
    "                else:\n",
    "                    weight_dict[(int(l[i]), int(l[j]))] += 1\n",
    "    #print(weight_dict)\n",
    "    setOfNodes = set(nodes)\n",
    "    print(len(setOfNodes))\n",
    "    \n",
    "    count = 0\n",
    "    for key, value in weight_dict.items():\n",
    "        if key[0]==1123 or key[0]==3269 or key[1]==1123 or key[1]==3269:\n",
    "            print(key[0], key[1])\n",
    "        if key[0] == key[1]:\n",
    "            print(key[0], key[1], value)\n",
    "        count += 1\n",
    "        o.write(f'{key[0]} {key[1]} {value}\\n')\n",
    "    \n",
    "    print(count)\n",
    "\n",
    "    test_data = t.readlines()\n",
    "    test_data = test_data[1:]\n",
    "\n",
    "    for line in test_data:\n",
    "        line = line.strip()\n",
    "        #print(line)\n",
    "        l = line.split(',')\n",
    "        for i in range(1, len(l)):   # Skip the index number in the first column.\n",
    "            if int(l[i]) not in setOfNodes:\n",
    "                setOfNodes.add(int(l[i]))\n",
    "                o.write(f'{int(l[i])} {int(l[i])} {0}\\n')\n",
    "                count+=1\n",
    "    nodes = list(setOfNodes)\n",
    "    #for node in nodes:\n",
    "    #    o.write(f'{node} {node} {0.01}\\n')\n",
    "    print('Added', count, ' edges.')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def link_prediction_split(nodes, weight_dict, files, portions):\n",
    "    \"\"\"\n",
    "    Divide a normal graph into a train split and several test splits for link prediction use.\n",
    "    Each test split contains half true and half false edges.\n",
    "\n",
    "    Parameters:\n",
    "        graph_file (str): graph file\n",
    "        files (list of str): file names,\n",
    "            the first file is treated as train file\n",
    "        portions (list of float): split portions\n",
    "    \"\"\"\n",
    "    assert len(files) == len(portions)\n",
    "    np.random.seed(1024)\n",
    "\n",
    "    nodes = set(nodes)\n",
    "    edges = []\n",
    "    for key,_ in weight_dict.items():\n",
    "        if key[0] == key[1]:\n",
    "            print(key[0], key[1])\n",
    "        edges.append(key)\n",
    "    \n",
    "    portions = np.cumsum(portions, dtype=np.float32) / np.sum(portions)\n",
    "    files = [open(file, \"w\") for file in files]\n",
    "    num_edges = [0] * len(files) # trian8， test2长度分别为多少\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    for line in edges:\n",
    "        u = line[0]\n",
    "        v = line[1]\n",
    "        i = np.searchsorted(portions, np.random.rand())\n",
    "        if i == 0:\n",
    "            if u==v:\n",
    "                print(u,v)\n",
    "            files[i].write(f'{u}\\t{v}\\t{weight_dict[(u,v)]}\\n')\n",
    "        else:\n",
    "            if u==v:\n",
    "                print(u,v)\n",
    "            files[i].write(f\"{u}\\t{v}\\t1\\n\")\n",
    "        num_edges[i] += 1\n",
    "\n",
    "    for node in nodes:\n",
    "        files[0].write(f'{node}\\t{node}\\t{0.01}\\n')\n",
    "        \n",
    "    nodes = list(nodes)\n",
    "    for file, num_edge in zip(files[1:], num_edges[1:]):\n",
    "        for _ in range(num_edge):\n",
    "            valid = False\n",
    "            while not valid:\n",
    "                u = nodes[int(np.random.rand() * len(nodes))]\n",
    "                v = nodes[int(np.random.rand() * len(nodes))]\n",
    "                valid = u != v and (u, v) not in edges and (v, u) not in edges\n",
    "            file.write(f\"{u}\\t{v}\\t0\\n\")\n",
    "    for file in files:\n",
    "        file.close()\n",
    "\n",
    "link_prediction_split(nodes, weight_dict, ['./data/train8.txt', './data/test2.txt'], [8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEzlCx4zs7cU"
   },
   "source": [
    "### NOTES On Colab\n",
    "#### When you run your code on colab you need to load the goole's virtual drive. Run the following blocks to load drive and 'cd' to the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19224,
     "status": "ok",
     "timestamp": 1617771789428,
     "user": {
      "displayName": "gong shuzhi",
      "photoUrl": "",
      "userId": "05618588835000088934"
     },
     "user_tz": -600
    },
    "id": "95DpLR0_rpE1",
    "outputId": "8d0054b9-1b41-4c70-b0e0-3347a6cc8419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1617771791131,
     "user": {
      "displayName": "gong shuzhi",
      "photoUrl": "",
      "userId": "05618588835000088934"
     },
     "user_tz": -600
    },
    "id": "aQwOsC6crafr",
    "outputId": "833aea1b-8ac2-413d-fc38-7541d5d03fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Graph_Link_Prediction/graphvite\n"
     ]
    }
   ],
   "source": [
    "cd drive/MyDrive/Graph_Link_Prediction/graphvite/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "feature_eng.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
